{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab_33_parallelization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parallelization Lab"
      ],
      "metadata": {
        "id": "NOdvOhH-mAnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, you will be leveraging several concepts you have learned to obtain a list of links from a web page and crawl and index the pages referenced by those links - both sequentially and in parallel. Follow the steps below to complete the lab."
      ],
      "metadata": {
        "id": "pjqWz0l1mD2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Use the requests library to retrieve the content from the URL below.**"
      ],
      "metadata": {
        "id": "MJ33t6SrmK80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/Data_science'"
      ],
      "metadata": {
        "id": "7ARG-k0cmGa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWn2cyMBlqFX",
        "outputId": "34511580-46a5-47d3-96f1-17d5c9cac08d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bytes"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "html = requests.get(url).content\n",
        "type(html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Use BeautifulSoup to extract a list of all the unique links on the page.**"
      ],
      "metadata": {
        "id": "UMUxXLq1mZqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "iSKuS3pBmi_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(html, \"lxml\");\n",
        "table = soup.find_all('a');\n",
        "\n",
        "links = [link['href'] for link in table if 'href' in link.attrs]"
      ],
      "metadata": {
        "id": "rbbH-lsQmjZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Use list comprehensions with conditions to clean the link list.**"
      ],
      "metadata": {
        "id": "ZwuePBaYmZUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two types of links, absolute and relative. Absolute links have the full URL and begin with http while relative links begin with a forward slash (/) and point to an internal page within the wikipedia.org domain. Clean the respective types of URLs as follows.\n",
        "\n",
        "Absolute Links: Create a list of these and remove any that contain a percentage sign (%).\n",
        "\n",
        "Relative Links: Create a list of these, add the domain to the link so that you have the full URL, and remove any that contain a percentage sign (%).\n",
        "\n",
        "Combine the list of absolute and relative links and ensure there are no duplicate"
      ],
      "metadata": {
        "id": "Crfjn8iImpdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domain = 'http://wikipedia.org'"
      ],
      "metadata": {
        "id": "TYunUXymmrvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "absolute_links =[link for link in links if (re.findall(r'http.+',link)) and ('%' not in link)]\n",
        "len(absolute_links)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjtbDrJImupu",
        "outputId": "ac60cedc-e464-468d-9da1-ad7c7f709e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relative_links = [domain+link for link in links if link.startswith('/') and ('%' not in link)]\n",
        "len(relative_links)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdWa4_UFtlAK",
        "outputId": "4551e86b-a559-4f64-a73e-3dc6eac3ac31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "290"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check if not duplicates"
      ],
      "metadata": {
        "id": "uH35NxBrwDTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_links = absolute_links + relative_links\n",
        "cleaned_links = list(set(cleaned_links))"
      ],
      "metadata": {
        "id": "LdnywJqYyRwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cleaned_links)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfFtfZdmxiUK",
        "outputId": "f90b185f-87cb-4fad-f3dc-bb57b624e002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "308"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4: Use the os library to create a folder called wikipedia and make that the current working directory.**"
      ],
      "metadata": {
        "id": "tUNpim1Nmvps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "KxdbGdpSm3Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = 'wikipedia'\n",
        "parent_dir = './'\n",
        "path = os.path.join(parent_dir, directory)\n",
        "path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IwNupg3Gm3nK",
        "outputId": "6a4ad10e-0a7d-4eaa-f3f2-7c535256be0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./wikipedia'"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(path)"
      ],
      "metadata": {
        "id": "f7UcXqQbz34P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "JsVEo1x09blo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5: Write a function called index_page that accepts a link and does the following.**"
      ],
      "metadata": {
        "id": "nbiu-XRVm6up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tries to request the content of the page referenced by that link.\n",
        "\n",
        "Slugifies the filename using the slugify function from the python-slugify library and adds a .html file extension.\n",
        "\n",
        "  If you don't already have the python-slugify library installed, you can pip install it as follows:\n",
        "  $ pip3 install python-slugify.\n",
        "\n",
        "  To import the slugify function, you would do the following: from slugify import slugify.\n",
        "\n",
        "  You can then slugify a link as follows slugify(link).\n",
        "\n",
        "Creates a file in the wikipedia folder using the slugified filename and writes the contents of the page to the file.\n",
        "\n",
        "If an exception occurs during the process above, just pass."
      ],
      "metadata": {
        "id": "RwdyZgAlnBj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from slugify import slugify"
      ],
      "metadata": {
        "id": "_wwWrNTjnEV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request, urllib.error, urllib.parse\n",
        "\n",
        "def index_page(link):\n",
        "  try:\n",
        "    file_name = slugify(link)+'.html'\n",
        "    html = urllib.request.urlopen(link)\n",
        "    webContent = html.read().decode('UTF-8')\n",
        "    with open(file_name, \"w\") as file:\n",
        "    file.write(webContent)\n",
        "   except:\n",
        "     pass"
      ],
      "metadata": {
        "id": "QEAwJx52nGjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for url in cleaned_links:\n",
        "    index_page(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LaUceman3kpM",
        "outputId": "f36fbf7c-bceb-4ed5-892d-d80890f49197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https-www-nsf-gov-pubs-2005-nsb0540'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6: Sequentially loop through the list of links, running the index_page function each time.**"
      ],
      "metadata": {
        "id": "OltqqX23nKYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to include %%time at the beginning of the cell so that it measures the time it takes for the cell to run."
      ],
      "metadata": {
        "id": "ffMFSKC5nO5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for url in urls:\n",
        "    index_page(url)"
      ],
      "metadata": {
        "id": "NsrfCNNqnR0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 7: Perform the page indexing in parallel and note the difference in performance.**"
      ],
      "metadata": {
        "id": "cjhzT8QAnUgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to include %%time at the beginning of the cell so that it measures the time it takes for the cell to run."
      ],
      "metadata": {
        "id": "S9W4gQMxnGqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing "
      ],
      "metadata": {
        "id": "HMCETwc5ncgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "pool = multiprocessing.Pool()\n",
        "result = pool.map(index_page, urls)\n",
        "pool.terminate()"
      ],
      "metadata": {
        "id": "FevDSdV_5Lx2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}